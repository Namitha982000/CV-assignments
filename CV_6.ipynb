{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e992f910-6ac7-4adf-baf1-ef9ae60d98f2",
   "metadata": {},
   "source": [
    "# Assignment 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab2b2904-c40b-408d-a5cd-c69bf697cc8d",
   "metadata": {},
   "source": [
    "#### 1. What is the difference between TRAINABLE and NON-TRAINABLE PARAMETERS?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b5a12e5-cdbd-4c3d-b1a4-946d8b897b62",
   "metadata": {},
   "source": [
    "1. **Trainable Parameters**: Trainable parameters, also known as learnable parameters, are the variables in a neural network that are updated or optimized during the training process. These parameters include weights and biases of the network. During training, the network learns the optimal values for these parameters by adjusting them based on the input data and the desired output. The purpose of training is to minimize a loss function, and the trainable parameters are updated iteratively using optimization algorithms such as gradient descent. By updating the trainable parameters, the network adapts and improves its performance on the given task.\n",
    "\n",
    "2. **Non-trainable Parameters**: Non-trainable parameters, also called fixed parameters or hyperparameters, are values or settings in a neural network that are predetermined and not updated during training. These parameters are set by the user or defined before the training process begins. Non-trainable parameters include aspects such as the network architecture (e.g., the number of layers, layer sizes), the choice of activation functions, learning rate, regularization techniques, and other hyperparameters that influence the behavior of the network. Non-trainable parameters are typically determined through trial and error, experimentation, or expert knowledge, and they are kept fixed during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a55e300-0e39-44fd-bd99-a1320cc881b3",
   "metadata": {},
   "source": [
    "#### 2. In the CNN architecture, where does the DROPOUT LAYER go?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b049942d-76ed-43bb-ba2b-c524612536fd",
   "metadata": {},
   "source": [
    "A Convolutional Neural Network (CNN)'s architecture commonly places the Dropout layer after one or more dense (completely linked) layers. By randomly dropping out (setting to zero) a portion of the neuron activations during training, the Dropout layer aims to avoid overfitting and enhance the network's capacity to generalise.\n",
    "\n",
    "In a CNN design, the Dropout layer is frequently placed where follows:\n",
    "\n",
    "1. **Convolutional Layers**: The CNN typically starts with a series of convolutional layers that extract hierarchical features from the input images. These convolutional layers consist of filters (kernels) that perform convolutions on the input to capture local patterns and features.\n",
    "\n",
    "2. **Pooling Layers**: After the convolutional layers, pooling layers, such as Max Pooling or Average Pooling, are often used to reduce the spatial dimensions of the feature maps and capture the most salient features.\n",
    "\n",
    "3. **Flattening**: Following the convolutional and pooling layers, the feature maps are flattened into a one-dimensional vector to be fed into the fully connected layers.\n",
    "\n",
    "4. **Fully Connected Layers**: The fully connected layers are responsible for learning high-level representations and making predictions based on the extracted features. It is common to insert the Dropout layer after one or more of these fully connected layers.\n",
    "\n",
    "5. **Output Layer**: The final layer of the CNN is the output layer, which produces the desired output based on the task at hand, such as classification probabilities or regression values.\n",
    "\n",
    "By inserting the Dropout layer after the fully connected layers, the network randomly drops out a fraction of the neuron activations during each training iteration. This helps to prevent the network from relying too heavily on specific features or connections, encouraging the network to learn more robust and generalized representations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cfd41f-3c46-4668-8959-e20b50c46c59",
   "metadata": {},
   "source": [
    "#### 3. What is the optimal number of hidden layers to stack?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7615c8-94b7-4f4f-8677-93bb1217b304",
   "metadata": {},
   "source": [
    "The ideal number of hidden layers to stack in a neural network relies on a variety of variables, including the difficulty of the task, the quantity of training data available, and the available computing power. The precise number of concealed layers that will function best in any situation cannot be determined in a one-size-fits-all manner. Here are some broad principles to take into account, though:\n",
    "\n",
    "1. **Start with a Simple Model**: It is often recommended to start with a simpler model, such as a shallow neural network with one or two hidden layers, and gradually increase the complexity if needed. This allows you to gauge the performance and complexity trade-off and identify if additional layers are necessary.\n",
    "\n",
    "2. **Complexity of the Problem**: The complexity of the problem you are trying to solve plays a crucial role in determining the number of hidden layers. More complex problems, such as object recognition in images or natural language processing, may require deeper networks with more hidden layers to capture intricate patterns and relationships in the data.\n",
    "\n",
    "3. **Available Training Data**: The size and quality of your training data also impact the number of hidden layers. If you have a small training dataset, having too many hidden layers may lead to overfitting, where the model becomes too specific to the training data and performs poorly on unseen data. In such cases, a shallower network with fewer hidden layers may be more appropriate.\n",
    "\n",
    "4. **Computational Resources**: The number of hidden layers also depends on the available computational resources. Deeper networks with more hidden layers require more computational power and training time. If you have limited resources, you may need to consider a simpler architecture with fewer hidden layers.\n",
    "\n",
    "5. **Regularization Techniques**: The use of regularization techniques, such as dropout or L2 regularization, can help prevent overfitting and improve generalization. With the inclusion of regularization, the need for an excessive number of hidden layers may decrease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1f0873d-be57-4f86-add4-2cbf917fdbfb",
   "metadata": {},
   "source": [
    "#### 4. In each layer, how many secret units or filters should there be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def7e113-f3b0-4477-9f36-e6db640a4662",
   "metadata": {},
   "source": [
    "The number of units or filters in a neural network's layers is a design decision that is influenced by the particular task at hand, the complexity of the data, and the available computing power. The appropriate number of units or filters in each layer cannot be determined with certainty because it depends on the specifics of the task at hand. Here are some broad ideas to keep in mind, though:\n",
    "\n",
    "\n",
    "1. **Input Layer**: The number of units in the input layer is determined by the dimensionality of the input data. Each unit in the input layer corresponds to a feature or attribute of the input.\n",
    "\n",
    "2. **Hidden Layers**: The number of units in the hidden layers is typically determined through experimentation and tuning. As a starting point, it is common to use a rule of thumb based on the number of inputs and outputs of the layer. For example, in feedforward neural networks, the number of units in each hidden layer can be set between the number of input units and the number of output units. However, this is not a strict rule and should be adjusted based on the complexity of the problem and the size of the training data.\n",
    "\n",
    "3. **Convolutional Layers**: In convolutional neural networks (CNNs), the number of filters in each convolutional layer determines the number of feature maps produced as output. The number of filters can be determined based on the complexity of the patterns or features that need to be detected. Typically, the number of filters starts small in the initial layers and increases gradually in deeper layers to capture more complex patterns.\n",
    "\n",
    "4. **Output Layer**: The number of units in the output layer is determined by the nature of the problem. For example, in classification tasks with multiple classes, the number of units in the output layer corresponds to the number of classes, with each unit representing the probability or score for a specific class."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c6eb777-b938-4341-a8ca-5584d9841dec",
   "metadata": {},
   "source": [
    "#### 5. What should your initial learning rate be?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31e4376-28e0-4a41-a989-8c1b374ed1b1",
   "metadata": {},
   "source": [
    "A crucial hyperparameter choice for neural network training is selecting an adequate initial learning rate. The challenge at hand, the network design, the dataset, and the optimisation technique being utilised are some of the variables that affect the ideal initial learning rate. There is no one size fits all solution, although the following broad principles can be used:\n",
    "\n",
    "1. **Rule of Thumb**: A common starting point for the initial learning rate is to use a value between 0.001 and 0.1. This range is often a reasonable choice and can provide a good starting point for experimentation.\n",
    "\n",
    "2. **Problem Complexity**: The complexity of the problem can influence the initial learning rate. More complex problems, such as fine-grained image classification or natural language processing, may require smaller learning rates to navigate the intricate landscape of the loss function and converge to a good solution. Conversely, simpler problems may tolerate higher learning rates.\n",
    "\n",
    "3. **Network Architecture and Depth**: The architecture and depth of the neural network can impact the learning rate. Deeper networks or architectures with a larger number of parameters may benefit from smaller learning rates to ensure stable convergence. Shallower networks or architectures with fewer parameters might tolerate higher learning rates.\n",
    "\n",
    "4. **Dataset Size**: The size of the dataset is another consideration. For smaller datasets, it is generally advisable to use smaller learning rates to prevent overfitting and ensure the model learns from the limited data more gradually. For larger datasets, higher learning rates can be used initially to speed up convergence.\n",
    "\n",
    "5. **Optimization Algorithm**: Different optimization algorithms, such as stochastic gradient descent (SGD) or adaptive optimizers like Adam or RMSprop, may require different learning rate choices. Adaptive optimizers often automatically adjust the learning rate during training, so their sensitivity to the initial learning rate might be lower compared to traditional SGD."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84d27c94-19a3-40fc-adc6-9948b04a656d",
   "metadata": {},
   "source": [
    "#### 6. What do you do with the activation function?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af42228-3387-44fe-b06e-578391c9cf7f",
   "metadata": {},
   "source": [
    "A neural network's activation function is an essential part since it introduces non-linearity and makes it possible for the network to recognise intricate patterns and connections in the data. Each neuron in a layer has its output subjected to the activation function, which modifies the input into the required range or format.\n",
    "\n",
    "You use the activation function primarily for the following purposes:\n",
    "\n",
    "1. **Introduce Non-Linearity**: The primary purpose of the activation function is to introduce non-linearity into the network. Without a non-linear activation function, the neural network would be limited to learning linear transformations of the input data, which would severely limit its representational power. By introducing non-linearity, the network becomes capable of learning and approximating more complex functions.\n",
    "\n",
    "2. **Learn Complex Patterns**: The activation function allows the neural network to learn complex patterns and relationships in the data. Non-linear activation functions such as Rectified Linear Unit (ReLU), sigmoid, or hyperbolic tangent (tanh) enable the network to model and capture non-linear patterns that may exist in the data. This helps the network to better discriminate between different classes or make accurate predictions.\n",
    "\n",
    "3. **Normalize Outputs**: Some activation functions, such as the sigmoid or softmax functions, can be used to normalize the outputs of a layer, particularly in classification tasks. The sigmoid function squashes the output values between 0 and 1, representing probabilities or binary decisions. The softmax function, commonly used in multi-class classification, normalizes the output values into a probability distribution over the classes, allowing the network to make class predictions.\n",
    "\n",
    "4. **Handle Vanishing and Exploding Gradients**: The choice of activation function can influence the occurrence of vanishing or exploding gradients during training. Activation functions like ReLU help mitigate the vanishing gradient problem by preventing the saturation of neuron activations and allowing gradients to flow more easily during backpropagation.\n",
    "\n",
    "5. **Consider Activation Function Selection**: There are various activation functions to choose from, each with its characteristics and suitable use cases. Common activation functions include ReLU, sigmoid, tanh, and softmax. The selection of the activation function depends on the nature of the problem, the type of output required, and the desired behavior of the network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a14b6c6-4ba7-4bd1-9c30-495a2140d900",
   "metadata": {},
   "source": [
    "#### 7. What is NORMALIZATION OF DATA?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd48f0d8-2918-4f70-93fa-d3a061bd0c9f",
   "metadata": {},
   "source": [
    "Data normalisation is the process of converting a dataset's values to a standardised range or distribution. To enable fair comparisons and improve the efficiency and convergence of machine learning algorithms, normalisation aims to scale down the dataset's features or variables to a common scale. When the features have multiple scales or units of measurement, normalisation is very crucial.\n",
    "\n",
    "Data normalisation can be done in a variety of ways, including:\n",
    "\n",
    "1. **Min-Max Scaling**: This method scales the values of the dataset to a fixed range, typically between 0 and 1. It is achieved by subtracting the minimum value of the feature and dividing it by the range (maximum minus minimum). The formula is:\n",
    "   ```\n",
    "   X_normalized = (X - X_min) / (X_max - X_min)\n",
    "   ```\n",
    "   This method preserves the relative relationships between the values but may be sensitive to outliers.\n",
    "\n",
    "2. **Standardization (Z-score normalization)**: This method transforms the values of the dataset to have a mean of 0 and a standard deviation of 1. It is achieved by subtracting the mean of the feature and dividing it by the standard deviation. The formula is:\n",
    "   ```\n",
    "   X_normalized = (X - mean) / standard_deviation\n",
    "   ```\n",
    "   Standardization centers the distribution of the data around zero and scales it using the standard deviation. It does not bound the values to a specific range and is less affected by outliers compared to min-max scaling.\n",
    "\n",
    "3. **Unit Vector Scaling**: This method scales each data point to have a unit norm (i.e., a length of 1) in the feature space. It is particularly useful when the magnitude of the data points is irrelevant, and only the direction or angle matters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47fbde1b-62c6-4bec-9de0-956531a53f02",
   "metadata": {},
   "source": [
    "#### 8. What is IMAGE AUGMENTATION and how does it work?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fea918db-5b25-439b-8f89-02b07437ab4b",
   "metadata": {},
   "source": [
    "By applying various alterations or perturbations to the original images, image augmentation is a technique used in computer vision and deep learning to artificially increase the size of a training dataset. In order to increase the training data's diversity and variability and help the model generalise and perform well on new or real-world data, picture augmentation is used.\n",
    "\n",
    "In order to create new copies of the original photos, a set of specified transformations or alterations are applied randomly to the images in image augmentation. Typical methods of picture enhancement include:\n",
    "\n",
    "1. **Horizontal/Vertical Flipping**: Images are flipped horizontally or vertically, simulating mirror images.\n",
    "\n",
    "2. **Rotation**: Images are rotated by a certain degree, introducing variations in orientation.\n",
    "\n",
    "3. **Translation**: Images are shifted horizontally or vertically, simulating movements.\n",
    "\n",
    "4. **Zooming**: Images are zoomed in or out, creating variations in scale.\n",
    "\n",
    "5. **Brightness/Contrast Adjustment**: The brightness or contrast of images is adjusted, altering their appearance.\n",
    "\n",
    "6. **Noise Addition**: Random noise is added to images, making them more robust to noise in real-world scenarios.\n",
    "\n",
    "7. **Crop and Resize**: Images are cropped or resized to different sizes, simulating different viewpoints or aspect ratios.\n",
    "\n",
    "During training, these adjustments are made at random to each image, resulting in brand-new pictures that show variations of the original data. The model can learn to generalise more effectively and become more resilient to different variances or distortions that may occur in real-world circumstances by supplementing the dataset with these altered photos.\n",
    "\n",
    "The overfitting phenomenon, in which the model gets overly specialised to the training data and performs badly on novel, unseen data, is prevented via image augmentation. The model performs better on test or validation datasets as a result of being exposed to a wider variety of image variations since it learns to identify and generalise patterns in various settings."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0f5dbd-be3c-402d-a1db-99c183d8d3c3",
   "metadata": {},
   "source": [
    "#### 9. What is DECLINE IN LEARNING RATE?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a34ecfee-1a78-4d08-ac0b-d52371b4f17b",
   "metadata": {},
   "source": [
    "The approach of progressively lowering the learning rate during the training phase of a machine learning model is referred to as the drop in learning rate, also known as learning rate decay or learning rate scheduling. In gradient-based optimisation methods like stochastic gradient descent (SGD), the updating step size is determined by the learning rate.\n",
    "\n",
    "The use of a falling learning rate is justified for a number of reasons:\n",
    "\n",
    "1. **Convergence**: A high learning rate at the beginning of training may cause the model to overshoot or oscillate around the optimal solution. By gradually reducing the learning rate, the model can converge towards a more optimal solution.\n",
    "\n",
    "2. **Stability**: A declining learning rate helps stabilize the training process, preventing large updates to the model parameters that can lead to instabilities or divergent behavior.\n",
    "\n",
    "3. **Refinement**: In later stages of training, smaller learning rates allow the model to make smaller, fine-grained adjustments to the parameters, helping to refine the learned representations.\n",
    "\n",
    "There are different strategies for implementing the decline in learning rate, including:\n",
    "\n",
    "1. **Step Decay**: The learning rate is reduced by a fixed factor or percentage after a certain number of epochs or iterations. For example, the learning rate may be halved every few epochs.\n",
    "\n",
    "2. **Exponential Decay**: The learning rate is decayed exponentially over time. The rate of decay can be controlled by a decay factor or a decay rate hyperparameter.\n",
    "\n",
    "3. **Time-based Decay**: The learning rate is reduced based on a predetermined schedule. For example, the learning rate may be decreased by a certain factor every fixed interval of time.\n",
    "\n",
    "4. **Performance-based Decay**: The learning rate is adjusted based on the model's performance on a validation set. If the performance plateaus or worsens, the learning rate is reduced to allow the model to escape local optima or refine the solution further.\n",
    "\n",
    "5. **Warm-up**: A warm-up strategy involves initially using a low learning rate and gradually increasing it to the desired value over a few epochs. This helps the model to stabilize and avoid large updates in the early stages of training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6fabbc1-66ce-4671-ade8-c4d172b28508",
   "metadata": {},
   "source": [
    "#### 10. What does EARLY STOPPING CRITERIA mean?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1f0ea8a-831a-47c7-8d73-1f786777a75b",
   "metadata": {},
   "source": [
    "Early stopping criteria are a method for deciding when to halt the training of machine learning models based on a predetermined criterion. Early stopping is used to avoid overfitting and determine the ideal point at which the model performs the best in terms of generalisation on unknown input.\n",
    "\n",
    "Typically, a validation set separate from the training set is used to track the model's performance throughout training. Based on performance measures (such as validation loss or accuracy) seen on this validation set, the early stopping conditions are established. The overall concept is to cease training when the model's performance on the validation set starts to deteriorate or plateau, however the criterion can be defined in a variety of ways.\n",
    "\n",
    "The main concept behind early stopping is that as the model continues to train, it initially improves its performance on both the training and validation sets. However, at some point, the model may start to overfit the training data, causing its performance on the validation set to degrade. Early stopping allows us to prevent further training and choose the model with the best performance before overfitting occurs.\n",
    "\n",
    "The specific criteria for early stopping can be defined based on different conditions, such as:\n",
    "\n",
    "1. **No improvement**: Training is stopped when there is no significant improvement in the model's performance on the validation set for a certain number of epochs or iterations.\n",
    "\n",
    "2. **Validation loss increase**: Training is stopped when the validation loss starts to increase continuously or exceeds a certain threshold, indicating that the model is overfitting.\n",
    "\n",
    "3. **Early stopping patience**: The training is stopped if there is no improvement in the validation performance for a specified number of epochs, known as the patience parameter.\n",
    "\n",
    "The early stopping criteria provide a mechanism to automatically find the point at which the model generalizes the best and prevents it from continuing to train on the training set excessively. By stopping the training early, we can avoid overfitting, save computational resources, and obtain a model that is more likely to perform well on new, unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d1cc5ab-6c2c-434f-823b-f4ccde2aecf2",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd9cfc9a-4aaa-494e-a73f-06e15ca8661b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
