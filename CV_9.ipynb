{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WkTuD00diQJ5",
        "outputId": "c7e16ae3-87d7-46e0-aebd-cca3a81f173f"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Assignment 9"
      ],
      "metadata": {
        "id": "Oxzh6VQ0jA3v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. What are the advantages of a CNN for image classification over a completely linked DNN?"
      ],
      "metadata": {
        "id": "QBFODO6djE6l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Convolutional Neural Networks (CNNs) have several advantages over completely linked Deep Neural Networks (DNNs) for image classification tasks:\n",
        "\n",
        "1. **Spatial Hierarchical Structure:** CNNs are designed to exploit the spatial structure of images. They preserve the spatial relationships between pixels by using convolutional layers, which scan the input with small receptive fields and learn local patterns. This spatial hierarchical structure enables CNNs to capture spatial dependencies and effectively model the visual patterns present in images.\n",
        "\n",
        "2. **Translation Invariance:** CNNs are invariant to translation, meaning they can recognize patterns regardless of their location in the image. This is achieved through the use of shared weights in convolutional layers, which enable the detection of the same pattern across different spatial locations. This translation invariance property is crucial for image classification tasks where the position of objects or features may vary.\n",
        "\n",
        "3. **Parameter Sharing:** CNNs leverage parameter sharing to reduce the number of trainable parameters. Instead of having separate weights for each pixel or neuron, CNNs use shared weights within each convolutional layer. This parameter sharing scheme significantly reduces the model's complexity, making it more efficient and scalable, especially for large-scale image datasets.\n",
        "\n",
        "4. **Local Receptive Fields:** CNNs use small receptive fields to capture local patterns in images. These local receptive fields enable CNNs to learn low-level features such as edges, corners, and textures, which are building blocks for higher-level representations. By progressively combining these local features, CNNs can learn more complex and abstract features, leading to better discrimination and understanding of the image content.\n",
        "\n",
        "5. **Pooling Layers:** CNNs incorporate pooling layers to downsample the spatial dimensions of the feature maps. Pooling operations, such as max pooling or average pooling, help to reduce the spatial resolution while retaining the most salient features. Pooling enhances the network's robustness to small spatial variations, reduces computational requirements, and introduces a degree of translation invariance.\n",
        "\n",
        "6. **Less Sensitivity to Input Variations:** CNNs are less sensitive to variations in the input images, such as changes in position, scale, or orientation. Due to the hierarchical nature of CNNs, higher-level features capture more abstract information, making them more invariant to specific variations. This property enables CNNs to handle images with different transformations or distortions more effectively."
      ],
      "metadata": {
        "id": "Fzmmc7GljEyV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 2. Consider a CNN with three convolutional layers, each of which has three kernels, a stride of two, and SAME padding. The bottom layer generates 100 function maps, the middle layer 200, and the top layer 400. RGB images with a size of 200 x 300 pixels are used as input. How many criteria does the CNN have in total? How much RAM would this network need when making a single instance prediction if we're using 32-bit floats? What if you were to practice on a batch of 50 images?"
      ],
      "metadata": {
        "id": "TR541hYQjErb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To calculate the number of parameters in the given CNN architecture, we need to consider the parameters in the convolutional layers and the fully connected layers (if any).\n",
        "\n",
        "1. **Convolutional Layers:**\n",
        "   - First Convolutional Layer: \n",
        "     - Number of kernels: 3\n",
        "     - Size of each kernel: 3 x 3\n",
        "     - Number of input channels: 3 (RGB images)\n",
        "     - Number of output feature maps: 100\n",
        "     - Total parameters in the first layer: 3 * 3 * 3 * 100 = 2700\n",
        "\n",
        "   - Second Convolutional Layer: \n",
        "     - Number of kernels: 3\n",
        "     - Size of each kernel: 3 x 3\n",
        "     - Number of input channels: 100\n",
        "     - Number of output feature maps: 200\n",
        "     - Total parameters in the second layer: 3 * 3 * 100 * 200 = 540,000\n",
        "\n",
        "   - Third Convolutional Layer: \n",
        "     - Number of kernels: 3\n",
        "     - Size of each kernel: 3 x 3\n",
        "     - Number of input channels: 200\n",
        "     - Number of output feature maps: 400\n",
        "     - Total parameters in the third layer: 3 * 3 * 200 * 400 = 2,880,000\n",
        "\n",
        "2. **Fully Connected Layers (if any):**\n",
        "   - The given CNN architecture does not mention any fully connected layers. If there are fully connected layers, the number of parameters would depend on the size of each layer.\n",
        "\n",
        "Therefore, the total number of parameters in the CNN is the sum of the parameters in each layer:\n",
        "2700 + 540,000 + 2,880,000 = 3,423,700 parameters.\n",
        "\n",
        "To calculate the RAM required for a single instance prediction and for a batch of 50 images, we need to consider the following:\n",
        "- Input size: RGB images with a size of 200 x 300 pixels.\n",
        "- Data type: 32-bit floats.\n",
        "\n",
        "1. **Single Instance Prediction:**\n",
        "   - Input size: 200 x 300 x 3 (RGB channels)\n",
        "   - RAM required for input: 200 * 300 * 3 * 4 bytes (32-bit float) = 720,000 bytes or 0.72 MB\n",
        "   - Additional RAM required for the model: Total number of parameters * 4 bytes (32-bit float)\n",
        "     = 3,423,700 * 4 bytes = 13,694,800 bytes or 13.69 MB\n",
        "   - Total RAM required: Input RAM + Model RAM = 0.72 MB + 13.69 MB = 14.41 MB\n",
        "\n",
        "2. **Batch of 50 Images:**\n",
        "   - Input size for each image: 200 x 300 x 3 (RGB channels)\n",
        "   - RAM required for input for a single image: 0.72 MB\n",
        "   - Total RAM required for the batch: Input RAM for a single image * Batch size = 0.72 MB * 50 = 36 MB\n",
        "   - Additional RAM required for the model: 13.69 MB\n",
        "   - Total RAM required: Input RAM for the batch + Model RAM = 36 MB + 13.69 MB = 49.69 MB\n",
        "\n",
        "Therefore, for a single instance prediction, the network would need approximately 14.41 MB of RAM, and for a batch of 50 images, it would require approximately 49.69 MB of RAM."
      ],
      "metadata": {
        "id": "O-Oj0X2djEjU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 3. What are five things you might do to fix the problem if your GPU runs out of memory while training a CNN?"
      ],
      "metadata": {
        "id": "p8ucj4a_jEYs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "If your GPU runs out of memory while training a CNN, here are five things you can do to address the issue:\n",
        "\n",
        "1. **Reduce Batch Size**: Decrease the batch size used during training. Batch size determines the number of samples processed in each iteration, and reducing it can help reduce the memory usage. However, a smaller batch size may also affect the convergence and overall performance of the model.\n",
        "\n",
        "2. **Resize or Crop Images**: If the input images are large, resizing them to a smaller resolution or cropping them can help reduce the memory requirements. This can be done while preprocessing the data before feeding it into the network.\n",
        "\n",
        "3. **Use Mixed Precision Training**: Utilize mixed precision training techniques, such as using half-precision (float16) instead of single-precision (float32) for storing activations and gradients. This can reduce the memory footprint of the model without significant loss in training quality.\n",
        "\n",
        "4. **Reduce Model Complexity**: Simplify the architecture of the CNN by reducing the number of layers, decreasing the number of filters, or employing other techniques to reduce the model's complexity. This can help reduce the memory usage and make the model more manageable on the available GPU memory.\n",
        "\n",
        "5. **Utilize Model Parallelism**: If your GPU has multiple GPUs or if you have access to distributed training frameworks, you can distribute the model across multiple GPUs. This approach, known as model parallelism, allows you to train larger models by splitting the model across multiple devices and distributing the computations."
      ],
      "metadata": {
        "id": "a_QRtFxsjSbZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4. Why would you use a max pooling layer instead with a convolutional layer of the same stride?"
      ],
      "metadata": {
        "id": "F7Uxp3aKjSWR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Dimensionality reduction**: Max pooling reduces the spatial dimensions (width and height) of the input feature maps while retaining the most prominent features. It achieves this by selecting the maximum value within each pooling region. This helps in reducing the computational complexity and the number of parameters in the network.\n",
        "\n",
        "2. **Translation invariance**: Max pooling provides a form of translation invariance by selecting the maximum activation within each pooling region. This means that even if the position of a feature slightly shifts within the pooling region, the maximum value will still be selected, preserving the essential information about the presence of that feature. This property helps the model to be more robust to small spatial variations and increases its ability to generalize.\n",
        "\n",
        "3. **Feature selection**: Max pooling acts as a form of feature selection by emphasizing the most important features while discarding less significant ones. By selecting the maximum activation within each pooling region, max pooling focuses on capturing the most salient features in the input feature maps.\n",
        "\n",
        "4. **Reduction of spatial overfitting**: Max pooling can help reduce the risk of overfitting by discarding unnecessary spatial details. It reduces the spatial resolution of the feature maps, making them less prone to overfitting on specific spatial patterns in the training data.\n",
        "\n",
        "5. **Computationally efficient**: Max pooling is computationally efficient compared to convolutional layers with the same stride. It requires fewer computations since it only selects the maximum value within each pooling region without performing any weight multiplication or convolution operation."
      ],
      "metadata": {
        "id": "s9Q1YcS1jSRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 5. When would a local response normalization layer be useful?"
      ],
      "metadata": {
        "id": "VXpKhwsTjSLR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. **Normalization of Local Neighborhood**: LRN normalizes the response of neurons within a local neighborhood. It helps to enhance the contrast between responses by normalizing the activations relative to their neighboring activations. This can be beneficial when you want to emphasize local contrast and highlight salient features in an image.\n",
        "\n",
        "2. **Lateral Inhibition**: LRN incorporates a form of lateral inhibition, where it suppresses the responses of neurons that are relatively weak compared to their neighbors. This inhibition mechanism can help create sparse representations and enhance the selectivity of neurons, promoting stronger responses for more pronounced features.\n",
        "\n",
        "3. **Normalization Across Channels**: LRN operates across channels, considering activations from different feature maps. By normalizing across channels, LRN encourages competition between different feature channels and reduces the dominance of highly activated channels, promoting diversity in the learned features.\n",
        "\n",
        "4. **Robustness to Brightness Variations**: LRN can provide some degree of robustness to brightness variations in the input data. By normalizing responses within a local neighborhood, LRN can help mitigate the impact of overall intensity changes in the image, making the network more invariant to such variations."
      ],
      "metadata": {
        "id": "BaUBE_FVjSDI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 6. In comparison to LeNet-5, what are the main innovations in AlexNet? What about GoogLeNet and ResNet's core innovations?"
      ],
      "metadata": {
        "id": "qDq3GOZjjRpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The main innovations in AlexNet compared to LeNet-5 are as follows:\n",
        "\n",
        "1. **Deeper architecture**: AlexNet introduced a much deeper architecture compared to LeNet-5, consisting of eight layers, including five convolutional layers and three fully connected layers. This deeper architecture allowed for more complex feature extraction and representation learning.\n",
        "\n",
        "2. **ReLU activation**: AlexNet replaced the sigmoid activation function used in LeNet-5 with the Rectified Linear Unit (ReLU) activation function. ReLU helps alleviate the vanishing gradient problem and accelerates the convergence of the network during training.\n",
        "\n",
        "3. **Data augmentation**: AlexNet employed data augmentation techniques such as image translations, horizontal reflections, and random cropping during training. This technique helped to increase the size of the training set and improve the network's ability to generalize.\n",
        "\n",
        "4. **Dropout regularization**: AlexNet introduced dropout regularization, a technique where randomly selected neurons are ignored during training. Dropout helps prevent overfitting by reducing co-adaptation among neurons and promoting better generalization.\n",
        "\n",
        "The core innovations of GoogLeNet are as follows:\n",
        "\n",
        "1. **Inception module**: GoogLeNet introduced the Inception module, which utilizes parallel convolutional filters of different sizes (1x1, 3x3, 5x5) to capture features at multiple scales. The outputs of these parallel filters are then concatenated to form the module's output. This architecture allows for efficient and effective feature extraction at various levels of abstraction.\n",
        "\n",
        "2. **Network depth**: GoogLeNet demonstrated the effectiveness of significantly increasing the network's depth without a proportional increase in computational complexity by using the Inception module. It showed that network depth plays a crucial role in improving performance.\n",
        "\n",
        "The core innovation of ResNet is:\n",
        "\n",
        "1. **Residual connections**: ResNet introduced residual connections, also known as skip connections, that bypass one or more layers in the network. This architecture enables the network to learn residual mappings, making it easier to optimize and train very deep networks. Residual connections help address the vanishing gradient problem and enable the network to learn more meaningful and complex representations."
      ],
      "metadata": {
        "id": "GdSGP0CCjgqS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 7. On MNIST, build your own CNN and strive to achieve the best possible accuracy."
      ],
      "metadata": {
        "id": "D3gh1WO8jjFr"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xfTusk_zccgA",
        "outputId": "8a946e39-2729-47c8-98ae-833c87c0551a"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 148328408.44it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 110525268.09it/s]\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 39915569.95it/s]"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 19184822.53it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Epoch 1/10 - Loss: 0.15250781043095074\n",
            "Epoch 2/10 - Loss: 0.044014206524121205\n",
            "Epoch 3/10 - Loss: 0.030127926049340736\n",
            "Epoch 4/10 - Loss: 0.024183074685952266\n",
            "Epoch 5/10 - Loss: 0.01692957489038949\n",
            "Epoch 6/10 - Loss: 0.014904003498827721\n",
            "Epoch 7/10 - Loss: 0.01009060187567396\n",
            "Epoch 8/10 - Loss: 0.008688662722807784\n",
            "Epoch 9/10 - Loss: 0.008405397762664564\n",
            "Epoch 10/10 - Loss: 0.00833569952428729\n",
            "Test Accuracy: 98.98%\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import MNIST\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# Define the CNN architecture\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu1 = nn.ReLU()\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)\n",
        "        self.relu2 = nn.ReLU()\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.relu3 = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.relu1(x)\n",
        "        x = self.pool(x)\n",
        "        x = self.conv2(x)\n",
        "        x = self.relu2(x)\n",
        "        x = self.pool(x)\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = self.fc1(x)\n",
        "        x = self.relu3(x)\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "train_dataset = MNIST(root='./data', train=True, download=True, transform=transform)\n",
        "test_dataset = MNIST(root='./data', train=False, download=True, transform=transform)\n",
        "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Create the CNN model\n",
        "model = CNN().to(device)\n",
        "\n",
        "# Define the loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "epochs = 10\n",
        "for epoch in range(epochs):\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch+1}/{epochs} - Loss: {running_loss/len(train_loader)}\")\n",
        "\n",
        "# Evaluation\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy*100}%\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 8. Using Inception v3 to classify broad images. a.\n",
        "Images of different animals can be downloaded. Load them in Python using the matplotlib.image.mpimg.imread() or scipy.misc.imread() functions, for example. Resize and/or crop them to 299 x 299 pixels, and make sure they only have three channels (RGB) and no transparency. The photos used to train the Inception model were preprocessed to have values ranging from -1.0 to 1.0, so make sure yours do as well.\n"
      ],
      "metadata": {
        "id": "nZ2r6rZcj2HY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.image as mpimg\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.inception_v3 import preprocess_input\n",
        "\n",
        "# Define the image paths\n",
        "image_paths = ['/content/drive/MyDrive/iNeuron Assignments/animals/c1.jpg', '/content/drive/MyDrive/iNeuron Assignments/animals/d2.jpg']\n",
        "\n",
        "# Create an empty list to store the preprocessed images\n",
        "preprocessed_images = []\n",
        "\n",
        "# Load and preprocess each image\n",
        "for image_path in image_paths:\n",
        "    # Load the image using matplotlib\n",
        "    img = mpimg.imread(image_path)\n",
        "    \n",
        "    # Resize the image to 299x299 pixels\n",
        "    img = tf.image.resize(img, (299, 299))\n",
        "    \n",
        "    # Convert the image to a numpy array\n",
        "    img = np.array(img)\n",
        "    \n",
        "    # Preprocess the image to have values ranging from -1.0 to 1.0\n",
        "    img = preprocess_input(img)\n",
        "    \n",
        "    # Append the preprocessed image to the list\n",
        "    preprocessed_images.append(img)\n",
        "\n",
        "preprocessed_images = np.array(preprocessed_images)\n",
        "\n",
        "model = tf.keras.applications.InceptionV3(weights='imagenet')\n",
        "\n",
        "# Make predictions on the preprocessed images\n",
        "predictions = model.predict(preprocessed_images)\n",
        "\n",
        "# Get the top predicted classes and their corresponding probabilities\n",
        "top_predictions = tf.keras.applications.imagenet_utils.decode_predictions(predictions, top=3)\n",
        "\n",
        "# Print the top predictions for each image\n",
        "for i, image_path in enumerate(image_paths):\n",
        "    print(\"Image:\", image_path)\n",
        "    for pred in top_predictions[i]:\n",
        "        print(\"Class:\", pred[1])\n",
        "        print(\"Probability:\", pred[2])\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jeiJcvUocrry",
        "outputId": "b4d2d81b-876b-4061-fc6c-6eb9dc2c8dbd"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_v3/inception_v3_weights_tf_dim_ordering_tf_kernels.h5\n",
            "96112376/96112376 [==============================] - 1s 0us/step\n",
            "1/1 [==============================] - 3s 3s/step\n",
            "Downloading data from https://storage.googleapis.com/download.tensorflow.org/data/imagenet_class_index.json\n",
            "35363/35363 [==============================] - 0s 0us/step\n",
            "Image: /content/drive/MyDrive/iNeuron Assignments/animals/c1.jpg\n",
            "Class: tiger_cat\n",
            "Probability: 0.45521465\n",
            "Class: tabby\n",
            "Probability: 0.39312863\n",
            "Class: Egyptian_cat\n",
            "Probability: 0.0993125\n",
            "Image: /content/drive/MyDrive/iNeuron Assignments/animals/d2.jpg\n",
            "Class: pug\n",
            "Probability: 0.8494927\n",
            "Class: Brabancon_griffon\n",
            "Probability: 0.011860026\n",
            "Class: bull_mastiff\n",
            "Probability: 0.0037340107\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 9. Large-scale image recognition using transfer learning.\n",
        "a. Make a training set of at least 100 images for each class. You might, for example, identify your own photos based on their position (beach, mountain, area, etc.) or use an existing dataset, such as the flowers dataset or MIT's places dataset (requires registration, and it is huge).\n",
        "\n",
        "b. Create a preprocessing phase that resizes and crops the image to 299 x 299 pixels while also adding some randomness for data augmentation.\n",
        "\n",
        "c. Using the previously trained Inception v3 model, freeze all layers up to the bottleneck layer (the last layer before output layer) and replace output layer with  appropriate number of outputs for your new classification task (e.g., the flowers dataset has five mutually exclusive classes so the output layer must have five neurons and use softmax activation function).\n",
        "\n",
        "d. Separate the data into two sets: a training and a test set. The training set is used to train the model, and the test set is used to evaluate it.\n"
      ],
      "metadata": {
        "id": "DZlgUBRBj7ny"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "To perform large-scale image recognition using transfer learning, you can follow these steps:\n",
        "\n",
        "a. Building the Training Set:\n",
        "   - Collect or obtain a training dataset with at least 100 images for each class you want to classify.\n",
        "   - You can capture your own photos or use existing datasets like the flowers dataset or MIT's places dataset.\n",
        "\n",
        "b. Preprocessing and Data Augmentation:\n",
        "   - Preprocess the images by resizing them to a consistent size, such as 299 x 299 pixels.\n",
        "   - Apply data augmentation techniques to add randomness and increase the diversity of the training data.\n",
        "   - Data augmentation techniques can include random cropping, flipping, rotation, zooming, and color jittering.\n",
        "\n",
        "c. Modifying the Inception v3 Model:\n",
        "   - Load the pretrained Inception v3 model.\n",
        "   - Freeze all the layers up to the bottleneck layer (the last layer before the output layer).\n",
        "   - Replace the output layer with an appropriate number of neurons for your classification task.\n",
        "   - Adjust the output layer to have the desired number of outputs, and use the softmax activation function for multi-class classification.\n",
        "\n",
        "d. Splitting the Data:\n",
        "   - Split the dataset into two sets: a training set and a test set.\n",
        "   - The training set is used to train the model, while the test set is used to evaluate its performance.\n",
        "   - Ensure that the data is split randomly and that each class is represented in both the training and test sets."
      ],
      "metadata": {
        "id": "0A-ziC2fj7j4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "EKHRdgRnj7fT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "LnteN1iNj7Zu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "rnSz047Dj7R_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "UyJcwYrIePFZ"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}