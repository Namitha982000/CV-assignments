{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f13a6c1d-26f6-41b1-b88b-0329c778a573",
   "metadata": {},
   "source": [
    "# Assignment 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924b5da1-dd42-48b5-b9ef-2a71dad93d66",
   "metadata": {},
   "source": [
    "#### 1. How can each of these parameters be fine-tuned? \n",
    "• Number of hidden layers \n",
    "• Network architecture (network depth)\n",
    "\n",
    "• Each layer's number of neurons (layer width)\n",
    "\n",
    "• Form of activation\n",
    "\n",
    "• Optimization and learning\n",
    "\n",
    "• Learning rate and decay schedule\n",
    "\n",
    "• Mini batch size\n",
    "\n",
    "• Algorithms for optimization\n",
    "\n",
    "• The number of epochs (and early stopping criteria)\n",
    "\n",
    "• Overfitting that be avoided by using regularization techniques.\n",
    "\n",
    "• L2 normalization\n",
    "\n",
    "• Drop out layers\n",
    "• Data augmentation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b80006-e7dc-42d0-8e26-2b77763d07c3",
   "metadata": {},
   "source": [
    "1. **Number of Hidden Layers**: The number of hidden layers can be adjusted to find the right balance between model complexity and the capacity to capture patterns in the data. Adding more hidden layers allows the network to learn more complex representations, but too many layers can lead to overfitting. It's often beneficial to start with a smaller number of hidden layers and gradually increase the complexity if needed.\n",
    "\n",
    "2. **Network Architecture (Network Depth)**: The overall architecture of the network, including the depth and structure of layers, can be fine-tuned. This involves experimenting with different arrangements of convolutional layers, pooling layers, and fully connected layers to find the optimal architecture for the given task.\n",
    "\n",
    "3. **Each Layer's Number of Neurons (Layer Width)**: The number of neurons or units in each layer can be adjusted to control the capacity of the network. Increasing the number of neurons allows the network to learn more complex representations, but it also increases the computational cost and risk of overfitting. Fine-tuning the layer width involves finding an appropriate balance between model capacity and generalization ability.\n",
    "\n",
    "4. **Form of Activation**: The choice of activation function in each layer can impact the network's ability to model non-linear relationships. Common activation functions include ReLU, sigmoid, and tanh. Fine-tuning the activation functions involves selecting appropriate functions for each layer or experimenting with novel activation functions.\n",
    "\n",
    "5. **Optimization and Learning**: Fine-tuning the optimization algorithm and learning parameters is crucial for efficient training. This includes selecting the appropriate optimization algorithm (e.g., stochastic gradient descent, Adam), setting the learning rate, and exploring learning rate schedules or adaptive learning rate methods to control the convergence and optimization process.\n",
    "\n",
    "6. **Mini-Batch Size**: The mini-batch size determines the number of samples processed in each iteration during training. Fine-tuning the mini-batch size involves experimenting with different batch sizes to strike a balance between computational efficiency and convergence speed. Larger batch sizes may provide more stable gradients but require more memory and computation.\n",
    "\n",
    "7. **Regularization Techniques**: To combat overfitting, various regularization techniques can be applied. L2 normalization (also known as weight decay) adds a penalty term to the loss function to encourage smaller weights. Dropout layers randomly set a fraction of the neuron activations to zero during training, preventing the network from relying too heavily on specific features or connections.\n",
    "\n",
    "8. **Data Augmentation**: Data augmentation techniques such as random rotations, translations, or flips can be applied to increase the diversity and variability of the training data. This helps to improve the network's ability to generalize to new, unseen data.\n",
    "\n",
    "9. **Number of Epochs and Early Stopping Criteria**: The number of training epochs, i.e., the number of times the entire dataset is processed during training, can be adjusted. Monitoring performance metrics on a validation set can help determine the optimal number of epochs. Early stopping criteria can be defined to stop training if the performance on the validation set stops improving, preventing overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932b3525-0734-4005-b261-a616920c16e8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
