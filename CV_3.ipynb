{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b947fdbd-d6be-42ee-a68e-2b0065599f1f",
   "metadata": {},
   "source": [
    "# Assignment 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd92f4bb-a446-467d-abfe-88d12a6ea1ad",
   "metadata": {},
   "source": [
    "#### 1. After each stride-2 conv, why do we double the number of filters?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00683c5e-9358-4b9f-a898-c35da3564472",
   "metadata": {},
   "source": [
    "In some network designs, especially in specific forms of convolutional neural networks (CNNs), doubling the number of filters after each stride-2 convolution is a typical practise. As the spatial dimensions of the feature maps shrink, this technique is frequently utilised to boost the capacity and richness of learned features.\n",
    "\n",
    "The stride-2 convolution reduces the feature map's spatial dimensions by a factor of two. The loss of spatial information and a decline in the richness of learnt features are both possible consequences of this reduction. To combat this, the network's capacity to collect a variety of features is maintained by doubling the number of filters, which also makes up for the decreased spatial resolution.\n",
    "\n",
    "There are a few reasons for doubling the number of filters after each stride-2 convolution:\n",
    "\n",
    "1. **Increased Capacity**: Doubling the number of filters increases the model's capacity to learn more complex and diverse features. With more filters, the network can capture a broader range of patterns and representations in the data.\n",
    "\n",
    "2. **Information Preservation**: Increasing the number of filters helps preserve the representational power of the network. By having a larger number of filters in the subsequent layers, the network can retain the ability to encode a wide variety of spatial information despite the reduction in spatial dimensions.\n",
    "\n",
    "3. **Compensation for Reduced Resolution**: Stride-2 convolutions reduce the spatial resolution, which can lead to information loss. Doubling the number of filters helps compensate for this loss by allowing more filters to cover the reduced spatial dimensions, capturing features at a higher level of abstraction.\n",
    "\n",
    "4. **Hierarchical Representation**: Doubling the number of filters after each stride-2 convolution contributes to creating a hierarchical representation of features. As the network goes deeper, it can capture increasingly complex patterns by combining information from multiple filters in previous layers.\n",
    "\n",
    "The network can keep its expressive capacity, adjust to smaller spatial dimensions, and capture a variety of interesting information by doubling the number of filters. This method is frequently used to enhance the representational capability and performance of CNNs in a variety of computer vision tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6e57fec-305e-4f89-a0ba-dc4f3d96ecaa",
   "metadata": {},
   "source": [
    "#### 2. Why do we use a larger kernel with MNIST (with simple cnn) in the first conv?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c24f2d60-8341-4095-8d87-8c72562a1518",
   "metadata": {},
   "source": [
    "To capture more local spatial information and more complex patterns in the input images, a bigger kernel is frequently used in the first convolutional layer of a straightforward CNN for the MNIST dataset.\n",
    "\n",
    "The handwritten digits in the MNIST collection are represented as 28x28 pixel grayscale pictures. The shapes in these photos are straightforward and clearly defined. However, there may still be differences in writing habits, stroke weights, and regional spatial configurations that add to the dataset's diversity.\n",
    "\n",
    "By using a larger kernel, such as a 5x5 or 7x7, in the first convolutional layer, the network can capture larger receptive fields and analyze the input images at a higher level of spatial detail. Here are some reasons why a larger kernel is beneficial:\n",
    "\n",
    "1. **Contextual Information**: A larger kernel size allows the network to consider a broader context within the image. It enables the network to capture more spatial relationships between pixels and recognize larger-scale patterns or structures.\n",
    "\n",
    "2. **Complex Patterns**: The larger kernel can help the network detect more complex patterns or combinations of features that might be present in the MNIST dataset. This is particularly important for distinguishing digits that share similar local features but differ in overall shape and structure.\n",
    "\n",
    "3. **Robustness to Variations**: A larger kernel can enhance the network's ability to generalize by capturing variations in writing styles, stroke thickness, and spatial arrangements of the digits. It helps the network learn features that are more robust to such variations and improves its performance on unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b006259e-37e0-4513-bbd8-8ddc396a7bac",
   "metadata": {},
   "source": [
    "#### 3. What data is saved by ActivationStats for each layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0819bbf-6f98-4bfc-ab13-8b082a1fa3d2",
   "metadata": {},
   "source": [
    "ActivationStats saves different data for each layer depending on the implementation or tool being used. To analyse and visualise the activations in a neural network during training or inference, ActivationStats is a tool or module that is generally utilised. It offers information about the statistics and traits of activations at various network layers.\n",
    "\n",
    "The information typically saved by ActivationStats for each layer may include:\n",
    "\n",
    "1. **Activation Histograms**: ActivationStats may store histograms of the activations for each layer. These histograms provide a distribution of activation values, allowing analysis of properties such as activation range, sparsity, or saturation.\n",
    "\n",
    "2. **Activation Statistics**: Basic statistical measures such as mean, standard deviation, minimum, and maximum values of the activations in each layer may be recorded. These statistics provide insights into the overall behavior and variability of activations.\n",
    "\n",
    "3. **Activation Visualizations**: ActivationStats may save visual representations of activations, such as heatmaps or feature maps, for each layer. These visualizations allow for qualitative analysis of how the activations change across different layers and provide a better understanding of the learned representations.\n",
    "\n",
    "4. **Activation Correlations**: In some cases, ActivationStats might calculate and store correlations between activations in different layers. This can help identify relationships and dependencies between layers and assess the flow of information through the network.\n",
    "\n",
    "5. **Layer Shapes**: ActivationStats may save the shapes or dimensions of the activation tensors in each layer. This information provides a structural overview of the activations at different network depths.\n",
    "\n",
    "6. **Layer Names or Identifiers**: To facilitate identification and organization, ActivationStats might associate each layer with a name or identifier to distinguish between different layers in the network.\n",
    "\n",
    "ActivationStats can save a variety of data, depending on the unique implementation and the analysis's goals. Researchers and practitioners can learn more about the behaviour and characteristics of activations during neural network training or inference using the handy tool ActivationStats, which aids in their understanding of how data is handled and converted across the network.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f143b72-98dd-4563-8836-67a0266b8c93",
   "metadata": {},
   "source": [
    "#### 4. How do we get a learner's callback after they've completed training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8d9060-c6e1-44a5-8cfe-858f0aca912a",
   "metadata": {},
   "source": [
    "You can make use of the callback mechanism offered by the deep learning framework or library you are using to obtain a learner's callback when they have finished training. Callbacks let you specify unique behaviour or actions at various training phases, including after training is over.\n",
    "\n",
    "Depending on the deep learning framework or library you're using, callbacks may be implemented differently than they are in actuality. However, the following is a general outline of how to do this:\n",
    "\n",
    "1. **Define your custom callback**: Create a custom callback class that inherits from the base callback class provided by the deep learning framework. This custom callback class will contain the specific behavior you want to execute after training completion.\n",
    "\n",
    "2. **Override the appropriate method**: Inside your custom callback class, override the method that is called when training is completed. In some frameworks, this method is typically called `on_train_end()` or similar. This method will contain the code you want to execute after training.\n",
    "\n",
    "3. **Register the callback**: During the setup of your training process, register an instance of your custom callback with the learner or training loop. This step will ensure that your callback is called at the appropriate stage.\n",
    "\n",
    "4. **Handle the callback**: When training completes, the deep learning framework will automatically call your custom callback's overridden method (`on_train_end()`). Inside this method, you can define the specific actions you want to perform, such as saving model weights, generating evaluation reports, or executing any other desired post-training tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4adf6069-7fb9-426b-bbc6-fc5678eabe89",
   "metadata": {},
   "source": [
    "#### 5. What are the drawbacks of activations above zero?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eee87262-f95b-4543-9c85-4ca309cee5ee",
   "metadata": {},
   "source": [
    "In neural networks, activations over zero typically have no inherent disadvantages. In actuality, gathering and spreading meaningful information throughout the network requires activations above zero. However, there are a few activations-related aspects that may be significant to note:\n",
    "\n",
    "1. **Saturation**: Activation functions such as the sigmoid or hyperbolic tangent (tanh) can become saturated as the inputs move away from zero. Saturation occurs when the activations approach the upper or lower bounds of the activation function (e.g., 1 or -1 for tanh). In saturated regions, the gradient of the activation function becomes close to zero, leading to vanishing gradients. This can hinder the learning process and make it difficult for the network to update the weights effectively.\n",
    "\n",
    "2. **Gradient Explosion**: In some cases, activations above zero can contribute to gradient explosion. When the gradients become very large during backpropagation, it can lead to unstable training and make it challenging for the network to converge.\n",
    "\n",
    "3. **Limited Dynamic Range**: If the activations are not appropriately scaled or normalized, activations above zero can lead to limited dynamic range. This can result in the loss of information in subsequent layers, as the activations may saturate or become concentrated around a narrow range of values.\n",
    "\n",
    "4. **Bias Shift**: Activations above zero can introduce bias in the network's predictions. If the network consistently produces positive activations, it may have a tendency to bias its predictions towards positive outcomes.\n",
    "\n",
    "Several methods can be used to address these problems, including gradient clipping to prevent gradient explosion, careful initialization of weights, proper normalisation of inputs and activations, and activation functions that reduce saturation, such as ReLU and Leaky ReLU.\n",
    "\n",
    "Overall, even though activations above zero are typically preferred for efficient information flow and learning in neural networks, it's critical to handle potential issues related to saturation, gradient explosion, a finite dynamic range, and bias shift to ensure stable and efficient training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4ab99f-667f-43b9-970a-4635c3e3595b",
   "metadata": {},
   "source": [
    "#### 6.Draw up the benefits and drawbacks of practicing in larger batches?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03b571bb-da46-4fe3-8590-dbf9e83dc131",
   "metadata": {},
   "source": [
    "Practicing in larger batches during training in deep learning has both benefits and drawbacks. Let's examine them:\n",
    "\n",
    "Benefits of practicing in larger batches:\n",
    "\n",
    "1. **Improved Computational Efficiency**: Training with larger batches allows for more efficient utilization of hardware resources, such as GPUs. Processing larger batches can exploit parallelism better, resulting in faster training times.\n",
    "\n",
    "2. **Stable Gradients**: Larger batch sizes often provide more stable gradient estimates compared to smaller batches. This stability can lead to smoother optimization and faster convergence to a good solution.\n",
    "\n",
    "3. **Improved Generalization**: Empirical evidence suggests that using larger batch sizes can improve generalization performance. It can help the model generalize well to unseen data by implicitly providing a form of regularization, smoothing the learned decision boundaries.\n",
    "\n",
    "4. **Better Utilization of Vectorized Operations**: Many deep learning frameworks and libraries are optimized for efficient matrix operations. Using larger batch sizes can better leverage these vectorized operations and achieve faster computations.\n",
    "\n",
    "Drawbacks of practicing in larger batches:\n",
    "\n",
    "1. **Higher Memory Requirements**: Larger batch sizes require more memory to store the input data, intermediate activations, and gradients. This can be a limitation, especially when dealing with large-scale models or limited GPU memory.\n",
    "\n",
    "2. **Reduced Model Flexibility**: Training with larger batches can limit the model's flexibility to adapt and update weights at a fine-grained level. Smaller batches can capture more stochasticity and finer details in the data, potentially leading to better exploration of the parameter space.\n",
    "\n",
    "3. **Possibility of Suboptimal Minima**: It has been observed that larger batch sizes can sometimes lead to convergence to suboptimal minima, resulting in slightly lower model performance. Smaller batches may allow the model to escape shallow local minima and find better solutions.\n",
    "\n",
    "4. **Longer Plateaus in Learning**: Larger batches may result in longer plateaus during training, where the model's progress appears slower as it needs to process more samples before making meaningful weight updates. This can lead to longer training times and slower convergence.\n",
    "\n",
    "5. **Loss of Fine-Grained Information**: Larger batch sizes can smooth out the noise or fine-grained information present in smaller batches. This loss of information may be detrimental in certain scenarios where subtle patterns or rare events need to be captured."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db61052-fa7a-438b-ac37-b970f4487607",
   "metadata": {},
   "source": [
    "#### 7. Why should we avoid starting training with a high learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04b65d47-c267-477a-8b70-1fb643efa6be",
   "metadata": {},
   "source": [
    "Starting training with a high learning rate can result in a number of problems that can impede the training process and impair the model's effectiveness. It is generally advised to refrain from beginning training with a high learning rate for the following reasons:\n",
    "\n",
    "1. **Instability and Divergence**: A high learning rate can cause the training process to become unstable. When the learning rate is too large, the optimization algorithm may overshoot the optimal solution and fail to converge. This can result in divergent behavior, where the loss function increases or oscillates instead of decreasing.\n",
    "\n",
    "2. **Difficulty in Convergence**: A high learning rate can make it difficult for the model to converge to an optimal solution. Large updates to the model parameters can cause them to constantly overshoot or oscillate around the optimal values, preventing the model from settling into a stable state.\n",
    "\n",
    "3. **Skipping Promising Regions**: A high learning rate can cause the optimization algorithm to quickly move through promising regions of the loss landscape without thoroughly exploring them. This can lead to missed opportunities to find better solutions and hinder the model's ability to generalize well to unseen data.\n",
    "\n",
    "4. **Inaccurate Gradient Estimation**: A high learning rate can amplify the effect of noise in the gradient estimates. This can lead to inaccurate updates of the model parameters and result in suboptimal solutions.\n",
    "\n",
    "5. **Difficulty in Fine-Tuning**: If the initial learning rate is too high, it can be challenging to fine-tune the model later in the training process. Fine-tuning typically requires smaller, more precise updates to fine-tune the model's performance, which can be difficult to achieve if the learning rate starts off too high.\n",
    "\n",
    "It is advised to begin training with a lower learning rate and progressively increase it over time, either manually or with the aid of learning rate schedules or adaptive optimisation algorithms, in order to minimise these problems. This method enables the model to make smaller, more gradual updates to the parameters in the beginning, assisting in the model's convergence to a better solution and stabilising the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e76bf945-8aa4-440a-b1d2-9f8a1bf84a53",
   "metadata": {},
   "source": [
    "#### 8. What are the pros of studying with a high rate of learning?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f463cc5a-b747-47a8-b2df-5f874ab88d9f",
   "metadata": {},
   "source": [
    "Studying quickly can offer some advantages, especially in specific situations or learning contexts. The following are some advantages of studying at a rapid rate of learning:\n",
    "\n",
    "1. **Rapid Initial Progress**: With a high learning rate, you can experience rapid initial progress in learning. The model parameters are updated more aggressively, leading to faster changes in the model's predictions and improved performance early on. This can be motivating and provide a sense of accomplishment, especially for tasks where quick progress is desired.\n",
    "\n",
    "2. **Exploration of Solution Space**: A high learning rate allows for more exploration of the solution space. By making large updates to the model parameters, the learning process can effectively jump over shallow local optima and explore a wider range of potential solutions. This can increase the chances of finding better solutions and escaping suboptimal regions.\n",
    "\n",
    "3. **Escape from Plateaus**: Plateaus in the loss landscape can hinder the learning process and slow down progress. With a high learning rate, the learning algorithm can overcome flat regions or long plateaus more easily by making large updates that push the parameters out of the stagnant region. This can help the learning process navigate towards more promising regions of the loss landscape.\n",
    "\n",
    "4. **Efficient for Simple Tasks**: High learning rates can be effective for simple tasks or when the data exhibits clear patterns. In such cases, the model can quickly converge to a good solution with a high learning rate, reducing the overall training time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb9293c-0bf9-465b-a2d1-bccd38d86970",
   "metadata": {},
   "source": [
    "#### 9. Why do we want to end the training with a low learning rate?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44f5e32e-af87-40b2-b8a1-e65178840988",
   "metadata": {},
   "source": [
    "Ending the training with a low learning rate can be beneficial for several reasons:\n",
    "\n",
    "1. **Refinement of Model Parameters**: Towards the end of training, the model parameters are often close to their optimal values. By using a low learning rate, the model can make small and precise updates to fine-tune the parameters and further improve the model's performance. This helps to refine the learned representations and make subtle adjustments to optimize the model's predictions.\n",
    "\n",
    "2. **Improved Convergence and Stability**: A low learning rate allows the optimization algorithm to carefully navigate the loss landscape and converge to a more optimal solution. It helps to stabilize the training process and avoid overshooting or oscillating around the optimal values. This can lead to better convergence behavior and more consistent model performance.\n",
    "\n",
    "3. **Avoidance of Catastrophic Forgetting**: Catastrophic forgetting refers to a situation where the model forgets previously learned information as it adapts to new information during training. Ending the training with a low learning rate helps mitigate this issue by allowing the model to retain the previously learned knowledge while fine-tuning the parameters with new information.\n",
    "\n",
    "4. **Better Generalization**: Using a low learning rate at the end of training can enhance the model's generalization performance. It allows the model to make small adjustments that help it generalize well to unseen data by reducing overfitting. The fine-tuning process with a low learning rate can improve the model's ability to capture subtle patterns and make more robust predictions on new examples.\n",
    "\n",
    "5. **Smoother Optimization Trajectory**: A low learning rate enables a smoother optimization trajectory towards the end of training. The smaller updates to the parameters result in a more gradual descent towards the optimal solution. This can help the model avoid getting stuck in sharp or noisy regions of the loss landscape and converge more smoothly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba9a535c-62a7-4967-869d-9c31e580ed09",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
